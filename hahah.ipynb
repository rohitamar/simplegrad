{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from Tensor import Tensor \n",
    "from Layers import Module, Linear, CrossEntropyLoss, Conv2d, MaxPool2d\n",
    "from Functional import Functional as F\n",
    "from optim import SGD \n",
    "from utils import one_hot_encode\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "    return data \n",
    "\n",
    "def load_data(filename):\n",
    "    d = load_pickle(filename)\n",
    "    data = d[\"data\"]\n",
    "    y = d['labels']\n",
    "    x = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        t = data[i]\n",
    "        r = np.reshape(t[0:1024], (32,32))\n",
    "        g = np.reshape(t[1024:2048], (32,32))\n",
    "        b = np.reshape(t[2048:4096], (32,32)) \n",
    "        x.append(np.dstack((r, g, b)))\n",
    "\n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    y = one_hot_encode(y, 10)\n",
    "\n",
    "    return np.array(x), y\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = Conv2d(32, 16, 3, padding=1)\n",
    "        self.pool = MaxPool2d(2, 2)\n",
    "        self.fc1 = Linear(16 * 4 * 4, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = Tensor.reshape(x, shape=(x.shape[0], -1))\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (64,32,3,32,3,3)->(64,32,3,newaxis,3,3,32) (16,3,3,3)->(16,3,3,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m batch_x, target \u001b[38;5;241m=\u001b[39m X_train[ind], y_train[ind]\n\u001b[0;32m     21\u001b[0m batch_x, target \u001b[38;5;241m=\u001b[39m Tensor(batch_x), Tensor(target)\n\u001b[1;32m---> 23\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, target)\n\u001b[0;32m     25\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\rohit\\Coding\\simplegrad\\Layers.py:18\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[4], line 44\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n",
      "File \u001b[1;32mc:\\Users\\rohit\\Coding\\simplegrad\\Layers.py:18\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rohit\\Coding\\simplegrad\\Layers.py:68\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohit\\Coding\\simplegrad\\Functional.py:126\u001b[0m, in \u001b[0;36mFunctional.conv2d\u001b[1;34m(input, filters)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_weight\u001b[39m(grad):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transpose_conv(grad, filters\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Tensor(\n\u001b[1;32m--> 126\u001b[0m     \u001b[43mcross_correlation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[0;32m    127\u001b[0m     [(\u001b[38;5;28minput\u001b[39m, apply_image), (filters, apply_weight)]\n\u001b[0;32m    128\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rohit\\Coding\\simplegrad\\Functional.py:99\u001b[0m, in \u001b[0;36mFunctional.conv2d.<locals>.cross_correlation\u001b[1;34m(input, kernel)\u001b[0m\n\u001b[0;32m     97\u001b[0m sub_matrices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mstride_tricks\u001b[38;5;241m.\u001b[39mas_strided(\u001b[38;5;28minput\u001b[39m, submatrices_shape, strides)\n\u001b[0;32m     98\u001b[0m sub_matrices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(sub_matrices, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m convolved \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbhwnij,onij->bhwo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m convolved \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(convolved, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m convolved\n",
      "File \u001b[1;32mc:\\Users\\rohit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\einsumfunc.py:1423\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[0;32m   1422\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c_einsum(\u001b[38;5;241m*\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1425\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (64,32,3,32,3,3)->(64,32,3,newaxis,3,3,32) (16,3,3,3)->(16,3,3,3) "
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data('./cifar_data/data_batch_1')\n",
    "X_test, y_test = load_data('./cifar_data/test_batch')\n",
    "X_test, y_test = Tensor(X_test), Tensor(y_test)\n",
    "\n",
    "model = Model()\n",
    "optim = SGD(model.parameters(), lr = 1e-3)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "test_every = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ind = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "\n",
    "    batch_x, target = X_train[ind], y_train[ind]\n",
    "    batch_x, target = Tensor(batch_x), Tensor(target)\n",
    "\n",
    "    pred = model(batch_x)\n",
    "    loss = criterion(pred, target)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    pred_class = np.argmax(pred.numpy(), axis=-1)\n",
    "    target_class = np.argmax(target.numpy(), axis=-1)\n",
    "    train_acc = (pred_class == target_class).mean()\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    if epoch % test_every == 0:\n",
    "        pred = model(X_test)\n",
    "        pred_class = np.argmax(pred.numpy(), axis=-1)\n",
    "        target_class = np.argmax(y_test.numpy(), axis=-1)\n",
    "        test_acc = (pred_class == target_class).mean()\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Training Accuracy: {train_acc} Testing Accuracy: {test_acc}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
